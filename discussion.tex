\chapter{Discussion}
\label{sec:discussion}
\minitoc
\vspace*{1cm}

In the discussion chapter, the limitations faced - such as a lack of fuzzers to compare with - during the development of \pname{} are outlined in detail and future what plans are being considered to upgrade our fuzzing tool are also highlighted.

\section{Limitations}
During the development of \pname{} we faced various obstacles that must be addressed to produce a more productive tool.

Our first major obstacle was the choice of Wfuzz as the main fuzzer to compare \pname{} with. After extensive research, it became clear that there are not as many black-box fuzzers available as there was a decade ago. Many older and renowned black-box fuzzers mentioned in various web sites and published papers ~\cite{doupe2010johnny, bau2010state, duchene2014kameleonfuzz} have either ceased to exist or are no longer developed and maintained. During our research, we discovered that Wfuzz is the only tool that can be imported as a module in Python, thereby extending its functionality. Although Wfuzz is classified as a 'brute-forcer', by providing the aforementioned functionality we can add code and make it operate as a black-box fuzzer. This enabled us to make a reasonable comparison with our fuzzing tool. Wfuzz was easier to use as it did not require time-consuming research and extra training. This can not be achieved with Burp Suite Professional or OWASP ZAP.

Additionally, during the evaluation phase, more evidence could be submitted to further explore the potential of \pname{} in detecting vulnerabilities. For instance, the tool can be evaluated on more open-source projects written in PHP and tackle other complex real-world XSS vulnerabilities, that will reflect real-world scenarios. A case in point, CVE is a good source of finding publicly-known XSS vulnerabilities. A recent paper by Backes et al. ~\cite{efficient2017} proposes ideas on such large-scale analysis of web application code to find real-world XSS bugs. We will use this as a reference point going forward. 

For the time being, webFuzz's vulnerabilities detection suite is limited to Reflected and Stored Cross-Site Scripting. DOMbased XSS vulnerabilities that rely on the browser's JavaScript runtime context, are beyond the fuzzer's scope. These types of attacks require no interaction with
the server, and succeed when the JS code does not sanitize the user input before rendering it unfiltered (e.g. using the {\tt innerHTML} property). For detecting these vulnerabilities, we would need to render the HTML and run the JavaScript code of each request. This would severely degrade the fuzzer's throughput, that is why this type of detection was not included for the initial version of \pname{}.  Unfortunately, by excluding JavaScript, due to a time deficit, many potential XSS vulnerabilities will go undetected.

\section{Future Work}
Our work is not yet done. Despite our initial accomplishments there is much we need to do to take this promising fuzzing tool to another, higher, level. Undoubtedly, improvements need to be made to ensure it is an effective and trustworthy tool. Below are my ideas for future progress.

There are future plans to include more functionalities in our tool kit to weed out other critical web-app vulnerabilities through our detection suite, so it can provide wider security protection that goes beyond Cross-Site Scripting. Such core vulnerabilities can be found at OWASP Top 10 ~\cite{owasp2017} the most common form of bug in web applications is Injection and Broken Authentication. Injection flaws, such as SQL and NoSQL, occur when untrusted data is sent to an interpreter or database as part of a query. For this specific vulnerability, various known payloads have already been collected ~\cite{seclist} - the same way as the XSS payloads are - and stored in the repository waiting for the respective functionality to be added to \pname{}.

There are also plans to implement a more efficient string-matching algorithm that will decrease the number of false positives we can currently record. This can be achieved by taking into consideration the location of the payload in the HTML document. These types of improvement will enable us to detect Cross-Site Scripting vulnerabilities that are triggered due to HTML attributes such as {\tt onchange } and {\tt onclick}, and not because of the HTML's <script>.

As we mentioned in the limitations, previous research has used techniques such as analysis of JavaScript code or Selenium-based crawlers to include the JavaScript-generated request URLs in their analysis. We could adopt similar approaches since we are currently missing many bugs excluding JavaScript. Moreover, to improve our evaluation we may adopt similar approaches that Backes et al. did  ~\cite{efficient2017} where they propose a way to build code property graphs for 1,854 popular open-source PHP projects on GitHub, storing them in a graph database and detect vulnerabilities through flow-finding traversals.

One idea on improving our fuzzer is that certain core functions of the
fuzzer might eventually be ported to faster languages; such as C and Java, that can substantially enhance speed performance and reduce memory consumption. Besides, a per link time-out will be introduced, to avoid I/O heavy web pages from stalling the fuzzing process. Initial work has also be done with netmap  ~\cite{rizzo2011Netmap}, a framework that modifies kernel modules to effectively bypass the Operating System's network stack, which often creates a bottleneck between client and server communication, and achieve a high speed packet I/O.

Also to be included, are more Python modules to improve the overall performance of \pname{}. Since our fuzzer requires a lot of file I/O to do its logging work, the {\tt mmap } module can be utilised by using lower-level operating system APIs to load a file directly into the computer memory and read/write files as if they were one large string or array ~\cite{mmap}. Another module that could boost the performance of \pname{} is aiomultiprocess ~\cite{aiomultiprocess}. As we briefly mentioned in Chapter ~\ref{sec:background}, AsyncIO is limited to the speed of GIL, and multiprocessing entails spreading tasks over a computer's cores. By combining the two, we can overcome these obstacles and truly achieve 'parallelism' in Python. Achieving 'parallelism' would be a beneficial outcome as today's PCs/laptops have processing units with multiple cores.

Having said these, ideas of optimization are one thing, putting them into practice is an entirely different matter. Every step has to be properly assessed and examined scientifically before they can be added to our tool.

\textit{"Premature optimization is the root of all evil (or at least most of it) in programming," said Donald Knuth - the father of algorithms analysis.}