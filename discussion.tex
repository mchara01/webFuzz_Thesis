\chapter{Discussion}
\label{sec:discussion}
\minitoc
\vspace*{1cm}

In the discussion chapter, any limitations faced during the development of \pname{} are stated and plans that we have for the future of our fuzzing tool.

\section{Limitations}
During the development of \pname{} we faced some obstacles that we discuss below.

Concerning the choice of Wfuzz as the main fuzzer to compare \pname{} with, after extensive research, it became apparent that there are not as many black-box fuzzers around as there used to be a decade ago. Many older and renowned black-box fuzzers mentioned in various web sites and published papers ~\cite{doupe2010johnny, bau2010state, duchene2014kameleonfuzz} have either ceased to exist or are no longer developed and maintained. We have found that Wfuzz is the only fuzzer that could easily be extended to our needs without requiring extensive research and training.

In addition, during the evaluation phase, more evidence can be submitted to explore deeper the potential of \pname{} in detecting vulnerabilities. For instance, the tool can be evaluated on more open-source projects written in PHP and on more complex real-world XSS vulnerabilities, that will reflect the real-world scenarios. Such large-scale analysis of web application code to find real-world XSS bugs where proposed by Backes et al. ~\cite{efficient2017}.

At the time being webFuzzâ€™s detection suite is limited to Reflected and Stored Cross-Site Scripting. DOMbased XSS vulnerabilities that rely on the browser's JavaScript runtime context, are so far out the fuzzer's scope. This kind of attacks require no interaction with
the server, and succeed when the JS code does not sanitize the user input before rendering it (e.g., using the {\tt innerHTML} property). For detecting these, we would have to render the HTML and run the JavaScript code of each request. This would in turn severely degrade the fuzzer's throughput, that is why it has been avoided for the initial version of \pname{}.  Unfortunately, by the complete exclusion of JavaScript many potential XSS vulnerabilities are missed.
 
Moreover, as I had no prior experience with fuzzing and the different categories that constitute it, I devoted much time at the start of thesis to educated my self at the work being done at research level in the field.


\section{Future Work}

Our work is not yet done. Despite our initial accomplishments there is much we need to do to take this promising fuzzing tool to another, higher, level. Undoubtedly, improvements need to be made to ensure it is an effective and trustworthy tool. Below are my ideas for future progress.

There are future plans to include more functionalities in our tool kit to weed out other critical web-app vulnerabilities through our detection suite, so it can provide wider security protection that goes beyond Cross-Site Scripting. Such core vulnerabilities can be found at OWASP Top 10 ~\cite{owasp2017} the most common form of bug in wed applications is Injection and Broken Authentication. Injection flaws, for instance, such as SQL and NoSQL, occur when untrusted data is sent to an interpreter or database as part of a query. For this specific vulnerability, various known payloads have already been collected ~\cite{seclist} - the same way as the XSS payloads are - and stored in the repository waiting for the respective functionality to be added to \pname{}.

There are also plans to implement a more efficient string-matching algorithm that will decrease the number of false positives we can currently record. This can be achieved by taking into consideration the location of the payload in the HTML document. These types of improvement will enable us to detect Cross-Site Scripting vulnerabilities that are triggered due to HTML attributes such as {\tt onchange } and {\tt onclick}, and not because of the HTML's <script>.

As we mentioned in the limitations, previous works have used techniques such as analysis of JavaScript code or Selenium-based crawlers in order to include the JavaScript-generated request URLs in their analysis. We can adopt similar approaches since we are currently missing many bugs be excluding JavaScript. %Moreover, to improve our evaluation we can adopt similar approaches that Backes et al. did  ~\cite{efficient2017} where they propose a way to built code property graphs for 1,854 popular open-source PHP projects on GitHub, storing them in a graph database and detect vulnerabilities through flow-finding traversals.

One idea on improving our fuzzer is that certain core functions of the
fuzzer might eventually be ported to faster languages; such as C and Java, that can substantially enhance speed performance and reduce memory consumption. Besides, a per link time-out will be introduced, to avoid I/O heavy web pages from stalling the fuzzing process. Initial work has also be done with netmap  ~\cite{rizzo2011Netmap}, a framework that modifies kernel modules to effectively bypass the Operating System's network stack, which often creates a bottleneck between client and server communication, and achieve a high speed packet I/O.

Also to be included, are more Python modules to improve the overall performance of \pname{}. Since our fuzzer requires a lot of file I/O to do its logging work, the {\tt mmap } module can be utilised by using lower-level operating system APIs to load a file directly into the computer memory and read/write files as if they were one large string or array ~\cite{mmap}. Another module that could boost the performance of \pname{} is aiomultiprocess ~\cite{aiomultiprocess}. As we briefly mentioned in Chapter ~\ref{sec:background}, AsyncIO is limited to the speed of GIL, and multiprocessing entails spreading tasks over a computer's cores. By combining the two, we can overcome these obstacles and truly achieve 'parallelism' in Python. Achieving 'parallelism' would be a beneficial outcome as today's PCs/laptops have processing units with multiple cores.
Having said this, ideas of optimization are one thing, putting them into practice is an entirely different matter. Every step has to be properly assessed and examined scientifically before they can be added to our tool.

\textit{"Premature optimization is the root of all evil (or at least most of it) in programming," said Donald Knuth - the father of the analysis of algorithms.}